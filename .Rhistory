p_final <- ggdraw(p_wc) +
# rectángulo blanco semitransparente
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x      = 0.885,        # posición relativa del centro del rectángulo
y      = 0.135,
width  = 0.23,
height = 0.25,
hjust  = 0.5, vjust = 0.5
) +
# texto con las 10 palabras más frecuentes
draw_text(
label      = top10_lbl,
x = 0.97,  y = 0.04,   # ancla esquina inferior derecha
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
###############################################################################
# Word-cloud for focus-group transcripts
# – Top 500 words, colour-blind palette
# – Bottom-right box with top-10 words and counts
# Run this from the project root (same level as “focus groups” folder)
###############################################################################
# 1. Packages -----------------------------------------------------------------
pkgs <- c("tidyverse", "tidytext", "ggwordcloud", "viridisLite",
"stopwords", "here", "cowplot", "grid")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Paths --------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Read transcripts and strip scaffolding -----------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # blank, line nums, timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># remove “Speaker: ”
paste(collapse = " ")
})
text_tbl <- tibble(text = clean_text)
# 4. Tokenise, remove stop-words, count ---------------------------------------
stop_snowball  <- tibble(word = stopwords::stopwords("en", source = "snowball"))
custom_fillers <- tibble(word = c("yeah","uh","um","like","youre","dont",
"im","thats","got","kind","sort"))
word_counts <- text_tbl |>
unnest_tokens(word, text) |>
mutate(word = str_replace_all(word, "[0-9]", ""),
word = str_trim(word)) |>
filter(str_length(word) > 2) |>
anti_join(bind_rows(stop_snowball, custom_fillers), by = "word") |>
count(word, sort = TRUE)
wc_plot_df <- word_counts |> slice_max(n, n = 500)           # keep top 500
# 5. Build label for top-10 table ---------------------------------------------
top10     <- word_counts |> slice_max(n, n = 10)
top10_lbl <- paste(sprintf("%-12s %5d", top10$word, top10$n), collapse = "\n")
# 6. Word-cloud ---------------------------------------------------------------
set.seed(123)
p_wc <- ggplot(wc_plot_df,
aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(eccentricity = 1) +
scale_size_area(max_size = 22) +
scale_colour_viridis_c(end = 0.85, guide = "none") +
theme_void()
# 7. Overlay white rectangle and text -----------------------------------------
p_final <- ggdraw(p_wc) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text       = top10_lbl,
x = 0.97,  y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 8. Save and display ----------------------------------------------------------
out_png <- file.path(out_dir, "wordcloud_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# End of script
###############################################################################
###############################################################################
# Word-cloud para transcripciones de focus-group
# – solo sustantivos y adjetivos con udpipe
# – top 400 palabras
# – paleta apta para daltónicos
# – recuadro con top-10 palabras y sus cuentas
# Ejecuta este script desde la raíz del proyecto
###############################################################################
# 1. Paquetes ------------------------------------------------------------------
pkgs <- c("tidyverse", "tidytext", "ggwordcloud", "viridisLite",
"stopwords", "here", "cowplot", "grid", "udpipe")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Rutas ---------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Leer transcripciones y limpiar encabezados --------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # líneas en blanco, números, timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># quita “Nombre: ”
paste(collapse = " ")
})
# 4. Cargar o descargar el modelo udpipe ---------------------------------------
model_path <- udpipe_download_model(language = "english")$file_model
model      <- udpipe_load_model(model_path)
# 5. Anotar texto y filtrar POS -------------------------------------------------
anno <- udpipe_annotate(model, x = paste(clean_text, collapse = " "))
anno <- as_tibble(anno)
content_words <- anno |>
filter(upos %in% c("NOUN", "ADJ")) |>    # solo sustantivos y adjetivos
mutate(token = tolower(token)) |>
count(token, sort = TRUE) |>
rename(word = token, n = n)
# 6. Preparar datos para la nube y el recuadro ---------------------------------
wc_plot_df <- content_words |> slice_max(n, n = 400)          # top-400
top10     <- content_words |> slice_max(n, n = 10)
top10_lbl <- paste(sprintf("%-12s %5d", top10$word, top10$n), collapse = "\n")
# 7. Construir la nube de palabras --------------------------------------------
set.seed(123)
p_wc <- ggplot(wc_plot_df,
aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(eccentricity = 1) +
scale_size_area(max_size = 22) +
scale_colour_viridis_c(end = 0.85, guide = "none") +
theme_void()
# 8. Superponer recuadro blanco y texto ----------------------------------------
p_final <- ggdraw(p_wc) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text       = top10_lbl,
x = 0.97,  y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 9. Guardar y mostrar ---------------------------------------------------------
if (!dir.exists(out_dir)) dir.create(out_dir)
out_png <- file.path(out_dir, "wordcloud_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# Fin del script
###############################################################################
###############################################################################
# Markov-chain graph (single words) for focus-group transcripts
# – keeps only nouns & adjectives via udpipe
# – restricts to top 30 words for clarity
# – embeds a top-10 frequency table
###############################################################################
# 1. Packages -----------------------------------------------------------------
pkgs <- c("tidyverse", "udpipe", "stopwords", "here",
"igraph", "ggraph", "viridisLite", "cowplot", "grid")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Paths --------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "output/figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Read & clean transcripts --------------------------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # blanks / nums / timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># strip “Name: ”
paste(collapse = " ")
})                                                   # one big string per file
# 4. Load / download udpipe model ------------------------------------------------
model_path <- udpipe_download_model(language = "english")$file_model
model      <- udpipe_load_model(model_path)
# 5. Annotation & word list ----------------------------------------------------
anno <- udpipe_annotate(model, x = paste(clean_text, collapse = " "))
anno <- as_tibble(anno)
content_words <- anno |>
filter(upos %in% c("NOUN", "ADJ")) |>
mutate(token = tolower(token)) |>
select(doc_id, paragraph_id, sentence_id, token_id, token)
# 6. Word frequencies ----------------------------------------------------------
word_freq <- content_words |>
count(token, sort = TRUE) |>
rename(word = token, freq = n)
# 7. Build transitions ---------------------------------------------------------
tokens_ordered <- content_words |>
arrange(doc_id, paragraph_id, sentence_id, token_id) |>
pull(token)
transitions <- tibble(
from = head(tokens_ordered, -1),
to   = tail(tokens_ordered, -1)
) |>
count(from, to, sort = TRUE) |>
rename(weight = n)
# 8. Limit to top 30 words for readability -------------------------------------
top_words <- word_freq |> slice_max(freq, n = 30) |> pull(word)
edges <- transitions |>
filter(from %in% top_words & to %in% top_words)
nodes <- word_freq |> filter(word %in% top_words)
# 9. Create igraph object ------------------------------------------------------
g <- graph_from_data_frame(edges, vertices = nodes, directed = TRUE)
# 10. Plot with ggraph ---------------------------------------------------------
set.seed(123)
p_graph <- ggraph(g, layout = "fr") +                       # force-directed layout
geom_edge_link(aes(width = weight),
arrow = arrow(length = unit(3, "mm")),
end_cap = circle(2, "mm"),
alpha = 0.6,
colour = viridisLite::viridis(1, begin = 0.1, end = 0.4)) +
scale_edge_width(range = c(0.3, 2), guide = "none") +
geom_node_point(aes(size = freq, colour = freq)) +
geom_node_text(aes(label = name), repel = TRUE, size = 3) +
scale_size(range = c(3, 12), guide = "none") +
scale_colour_viridis_c(option = "D", end = 0.85, guide = "none") +
theme_void()
# 11. Build top-10 table text ---------------------------------------------------
top10 <- word_freq |> slice_max(freq, n = 10)
top10_lbl <- paste(sprintf("%-12s %5d", top10$word, top10$freq), collapse = "\n")
# 12. Overlay frequency table ---------------------------------------------------
p_final <- ggdraw(p_graph) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text = top10_lbl,
x = 0.97, y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 13. Save & display -----------------------------------------------------------
out_png <- file.path(out_dir, "markov_words_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# End of script
###############################################################################
###############################################################################
# Markov-chain graph (overlapping 2-grams) for focus-group transcripts
# – keeps nouns & adjectives only (udpipe)
# – shows top 30 bigrams
# – embeds top-10 bigram frequency table
###############################################################################
# 1. Packages -----------------------------------------------------------------
pkgs <- c("tidyverse", "udpipe", "here", "igraph", "ggraph",
"viridisLite", "cowplot", "grid")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Paths --------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Read & clean transcripts --------------------------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # blank / nums / timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># strip “Name: ”
paste(collapse = " ")
})
# 4. Load / download udpipe model ---------------------------------------------
model_path <- udpipe_download_model(language = "english")$file_model
model      <- udpipe_load_model(model_path)
# 5. Annotation & token list ---------------------------------------------------
anno <- udpipe_annotate(model, x = paste(clean_text, collapse = " "))
anno <- as_tibble(anno)
tokens <- anno |>
filter(upos %in% c("NOUN", "ADJ")) |>
mutate(token = tolower(token)) |>
arrange(doc_id, paragraph_id, sentence_id, token_id) |>
pull(token)
# 6. Build overlapping bigrams & counts ---------------------------------------
bigrams <- paste(head(tokens, -1), tail(tokens, -1))
bigram_freq <- tibble(bigram = bigrams) |>
count(bigram, sort = TRUE)
# 7. Transitions between consecutive bigrams -----------------------------------
bigram_trans <- tibble(
from = head(bigrams, -1),
to   = tail(bigrams, -1)
) |>
count(from, to, sort = TRUE) |>
rename(weight = n)
# 8. Keep top 30 bigrams for clarity -------------------------------------------
top_bigrams <- bigram_freq |> slice_max(n, n = 30) |> pull(bigram)
edges <- bigram_trans |>
filter(from %in% top_bigrams & to %in% top_bigrams)
nodes <- bigram_freq |>
filter(bigram %in% top_bigrams) |>
rename(name = bigram, freq = n)
# 9. Build igraph & plot -------------------------------------------------------
g <- graph_from_data_frame(edges, vertices = nodes, directed = TRUE)
set.seed(123)
p_graph <- ggraph(g, layout = "fr") +
geom_edge_link(aes(width = weight),
arrow = arrow(length = unit(3, "mm")),
end_cap = circle(2, "mm"),
alpha = 0.6,
colour = viridisLite::viridis(1, begin = 0.1, end = 0.4)) +
scale_edge_width(range = c(0.3, 2), guide = "none") +
geom_node_point(aes(size = freq, colour = freq)) +
geom_node_text(aes(label = name), repel = TRUE, size = 3) +
scale_size(range = c(3, 12), guide = "none") +
scale_colour_viridis_c(option = "D", end = 0.85, guide = "none") +
theme_void()
# 10. Top-10 bigram table -------------------------------------------------------
top10 <- bigram_freq |> slice_max(n, n = 10)
top10_lbl <- paste(sprintf("%-20s %5d", top10$bigram, top10$n), collapse = "\n")
# 11. Overlay table ------------------------------------------------------------
p_final <- ggdraw(p_graph) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text = top10_lbl,
x = 0.97, y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 12. Save & display -----------------------------------------------------------
out_png <- file.path(out_dir, "markov_bigrams_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# End of script
###############################################################################
###############################################################################
# Word-cloud para transcripciones de focus-group
# – solo sustantivos y adjetivos con udpipe
# – top 400 palabras
# – paleta apta para daltónicos
# – recuadro con top-10 palabras y sus cuentas
# Ejecuta este script desde la raíz del proyecto
###############################################################################
# 1. Paquetes ------------------------------------------------------------------
pkgs <- c("tidyverse", "tidytext", "ggwordcloud", "viridisLite",
"stopwords", "here", "cowplot", "grid", "udpipe")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Rutas ---------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "output/figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Leer transcripciones y limpiar encabezados --------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # líneas en blanco, números, timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># quita “Nombre: ”
paste(collapse = " ")
})
# 4. Cargar o descargar el modelo udpipe ---------------------------------------
model_path <- udpipe_download_model(language = "english")$file_model
model      <- udpipe_load_model(model_path)
# 5. Anotar texto y filtrar POS -------------------------------------------------
anno <- udpipe_annotate(model, x = paste(clean_text, collapse = " "))
anno <- as_tibble(anno)
content_words <- anno |>
filter(upos %in% c("NOUN", "ADJ")) |>    # solo sustantivos y adjetivos
mutate(token = tolower(token)) |>
count(token, sort = TRUE) |>
rename(word = token, n = n)
# 6. Preparar datos para la nube y el recuadro ---------------------------------
wc_plot_df <- content_words |> slice_max(n, n = 400)          # top-400
top10     <- content_words |> slice_max(n, n = 10)
top10_lbl <- paste(sprintf("%-12s %5d", top10$word, top10$n), collapse = "\n")
# 7. Construir la nube de palabras --------------------------------------------
set.seed(123)
p_wc <- ggplot(wc_plot_df,
aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(eccentricity = 1) +
scale_size_area(max_size = 22) +
scale_colour_viridis_c(end = 0.85, guide = "none") +
theme_void()
# 8. Superponer recuadro blanco y texto ----------------------------------------
p_final <- ggdraw(p_wc) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text       = top10_lbl,
x = 0.97,  y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 9. Guardar y mostrar ---------------------------------------------------------
if (!dir.exists(out_dir)) dir.create(out_dir)
out_png <- file.path(out_dir, "wordcloud_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# Fin del script
###############################################################################
###############################################################################
# Markov-chain graph (overlapping 2-grams) for focus-group transcripts
# – keeps nouns & adjectives only (udpipe)
# – shows top 30 bigrams
# – embeds top-10 bigram frequency table
###############################################################################
# 1. Packages -----------------------------------------------------------------
pkgs <- c("tidyverse", "udpipe", "here", "igraph", "ggraph",
"viridisLite", "cowplot", "grid")
new <- pkgs[!pkgs %in% installed.packages()[ , "Package"]]
if (length(new) > 0) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# 2. Paths --------------------------------------------------------------------
root_path  <- here::here()
focus_path <- file.path(root_path, "focus groups")
out_dir    <- file.path(root_path, "output/figures")
if (!dir.exists(out_dir)) dir.create(out_dir)
# 3. Read & clean transcripts --------------------------------------------------
txt_files <- list.files(focus_path, "\\.txt$", full.names = TRUE)
clean_text <- map_chr(txt_files, function(path) {
readr::read_lines(path) |>
discard(~ str_detect(.x, "^\\s*$|^\\d+$|-->")) |>        # blank / nums / timestamps
map_chr(~ str_replace(.x, "^\\s*[^:]{1,40}:\\s*", "")) |># strip “Name: ”
paste(collapse = " ")
})
# 4. Load / download udpipe model ---------------------------------------------
model_path <- udpipe_download_model(language = "english")$file_model
model      <- udpipe_load_model(model_path)
# 5. Annotation & token list ---------------------------------------------------
anno <- udpipe_annotate(model, x = paste(clean_text, collapse = " "))
anno <- as_tibble(anno)
tokens <- anno |>
filter(upos %in% c("NOUN", "ADJ")) |>
mutate(token = tolower(token)) |>
arrange(doc_id, paragraph_id, sentence_id, token_id) |>
pull(token)
# 6. Build overlapping bigrams & counts ---------------------------------------
bigrams <- paste(head(tokens, -1), tail(tokens, -1))
bigram_freq <- tibble(bigram = bigrams) |>
count(bigram, sort = TRUE)
# 7. Transitions between consecutive bigrams -----------------------------------
bigram_trans <- tibble(
from = head(bigrams, -1),
to   = tail(bigrams, -1)
) |>
count(from, to, sort = TRUE) |>
rename(weight = n)
# 8. Keep top 30 bigrams for clarity -------------------------------------------
top_bigrams <- bigram_freq |> slice_max(n, n = 30) |> pull(bigram)
edges <- bigram_trans |>
filter(from %in% top_bigrams & to %in% top_bigrams)
nodes <- bigram_freq |>
filter(bigram %in% top_bigrams) |>
rename(name = bigram, freq = n)
# 9. Build igraph & plot -------------------------------------------------------
g <- graph_from_data_frame(edges, vertices = nodes, directed = TRUE)
set.seed(123)
p_graph <- ggraph(g, layout = "fr") +
geom_edge_link(aes(width = weight),
arrow = arrow(length = unit(3, "mm")),
end_cap = circle(2, "mm"),
alpha = 0.6,
colour = viridisLite::viridis(1, begin = 0.1, end = 0.4)) +
scale_edge_width(range = c(0.3, 2), guide = "none") +
geom_node_point(aes(size = freq, colour = freq)) +
geom_node_text(aes(label = name), repel = TRUE, size = 3) +
scale_size(range = c(3, 12), guide = "none") +
scale_colour_viridis_c(option = "D", end = 0.85, guide = "none") +
theme_void()
# 10. Top-10 bigram table -------------------------------------------------------
top10 <- bigram_freq |> slice_max(n, n = 10)
top10_lbl <- paste(sprintf("%-20s %5d", top10$bigram, top10$n), collapse = "\n")
# 11. Overlay table ------------------------------------------------------------
p_final <- ggdraw(p_graph) +
draw_grob(
grid::rectGrob(gp = gpar(fill = "white", col = NA, alpha = 0.85)),
x = 0.885, y = 0.135, width = 0.23, height = 0.25,
hjust = 0.5, vjust = 0.5
) +
draw_text(
text = top10_lbl,
x = 0.97, y = 0.04,
hjust = 1, vjust = 0,
fontfamily = "mono", size = 8,
lineheight = 1.05
)
# 12. Save & display -----------------------------------------------------------
out_png <- file.path(out_dir, "markov_bigrams_focus_groups.png")
ggsave(out_png, p_final, width = 8, height = 6, dpi = 320, bg = "white")
print(p_final)
###############################################################################
# End of script
###############################################################################
